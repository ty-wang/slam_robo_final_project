# slam_robo_final_project
Code for CMU 16833 Robot Localization and Mapping final project (F19) by Yibo Cao, Yijun Luo, and Tianyu Wang

Although state-of-the-art visual odometry algorithms have shown excellent performance in modern SLAM systems, they still suffer from heavy parameter tuning processes while being implemented in completely new environments. Deep learning based visual odometry has been seen as a candidate for solving this problem and adding accuracy and robustness to visual odometry modules in SLAM systems. In this project we make a comparison of an end-to-end deep learning based visual odometry with a traditional geometry based visual odometry in ORB-SLAM pipeline. DeepVO leverages the advantages of CNNs and RNNs on image processing and neighbouring information co-relating. It has the advantage of only using raw RGB images as input without any other carefully tuned parameters, such as camera calibration. While the visual odometry of ORB-SLAM2 uses corner detector to extract features for bag of words method, then it applies camera calibration and careful parameter tuning for pose estimation. By testing both visual odometry module in ORB-SLAM system and the deep learning based visual odometry algorithm on KITTI dataset, we found that the deep learning based visual odometry still cannot outperform the conventional visual odometry methods. But we believe that the deep learning based visual odometry can be used as a good complementary of the conventional methods to improve its robustness in different environments.
